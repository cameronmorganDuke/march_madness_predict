# march_madness_predict
Investigating the Most Effective Metrics in Predicting March Madness Success
Group members: Dom Fenoglio: djf49, Cameron Morgan: cwm36, Ricardo Ureña: ru15, Chris Johnson: cgj17
Box folder link: https://duke.box.com/s/wdshw22i3l0rqc2bgls0amge5pa3n098
Section 1: Introduction and Research Questions
Recently, Duke men’s basketball competed in the NCAA men’s basketball tournament, making it all the way to the Elite Eight before bowing out to N.C. State — the biggest win of a massive Cinderella run for the Wolfpack. They were the surprise of this year’s tournament, but March Madness, the colloquial name for the NCAA men’s basketball tournament each spring, is known for its massive upsets and unpredictability. What if we could try to find a method to the madness? In other words, what factors best predict a team’s postseason success, ranging from offensive and defensive efficiency to tempo and free-throw shooting? Our research questions are as follows:
What factors are most correlated with success in March Madness?
What factors are overvalued in predicting March Madness success?
To what degree is it possible to predict a team’s success in March Madness?
Our group wants to examine team statistics in college basketball in order to answer this question and learn more about why certain teams are successful. Specifically from our data sets, we want to look at adjusted tempo (the number of possessions a team gets a game), offensive and defensive efficiency, 3-point percentage, 2-point percentage, rebounding, and a metric called BARTHAG (the probability of beating an average college team).
We were able to pull data from the past seven tournaments and filter it so the data we used is from before March Madness began — so that in-tournament performance does not skew our data. This means that we can widen our scope to see why teams make the NCAA tournament in the first place. We also found a dataset that contains the results of each team’s performance in the tournament, including how many games they won.
We believe our research will be relevant because we aim to discover what makes a college basketball team successful, which can have important consequences in the realms of sports betting, coaching, and player and team development. Moreover, this type of analysis and project would be interesting and relevant to the student body here at Duke, who strongly supports the basketball team to the point of camping for games.
For the final project, we continued the work we started in our prototype and introduced new forms of analysis. In our prototype, we only used data from the past five tournaments, but we increased our sample to the past seven years in order to gather more data. This is especially important when looking at things like making the Final Four, as each year added allows us to see four more data points of teams that reached that point in the tournament. We tried to fit a K-nearest neighbors (KNN) model that took a subset of our data and tried to predict whether those teams would make the Final four. We primarily used figures and charts to answer research questions 1 and 2, but the KNN model helped us answer research question 3 and give us more evidence for our answers to questions 1 and 2.

Section 2: Data Sources
The bulk of our data was taken from Barttorvik.com. Bart Torvik contains a variety of downloadable CSV files, ranging from individual game stats to team performance over the course of the season. We looked at multiple T-Rank table data sets, which gives many statistics about every team in division one college basketball, Some that we looked at are adjusted offensive efficiency, which is the amount of points a team scores per 100 possessions adjusted against an average D1 opponent, adjusted defensive efficiency, which is the amount of points a team allows per 100 possession against an average D1 opponent,  and adjusted tempo, which is the amount of possessions a team has per game adjusted against an average tempo D1 opponent. We also looked at two point percentage (percent of made twos over a season), three point percentage (percent of made threes over a season), and many more. We took seven T-Rank table csv files from the last seven years of college basketball (not including 2020 as there was no tournament that year). We filtered by Regular Season on Barttorvik.com to get data that was at the end of the season, but before the start of the tournament. This way, how a team performs during the tournament does not affect any of the statistics, and one could use our analysis to predict how a team will perform before the tournament begins. We merged this data together with 538’s tournament ratings data set, to give us a complete understanding of how the teams performed in the tournament. We paid particular attention to the ‘ROUND’ variable, which was set as 1 if a team won the championship, 2 if they made the championship game but lost it, 4 if they lost in the final four, and so on.
Bart Torvik tables: https://www.barttorvik.com/trank.php 
538 Tournament Ratings and Results:
https://www.kaggle.com/datasets/nishaanamin/march-madness-data?select=538+Ratings.csv

Section 3: What modules are we using?
Data Wrangling
Data wrangling was the first step of our data analysis pipeline, facilitating the transformation and preparation of raw data into a format suitable for exploration and modeling. Our data wrangling process encompasses several key steps aimed at ensuring data quality and consistency across different datasets. First, cleaned the data to make sure there were no errors or missing values present in the datasets. We dropped columns that were redundant or did not contribute to our analysis. By streamlining the dataset and focusing on relevant features, we streamlined subsequent analysis tasks.
Moreover, data wrangling involves the creation of new columns to enhance the informational richness of the dataset. In our analysis, we introduced two new columns to capture critical aspects of team performance in March Madness: one indicating whether a team made it to the Final Four and another denoting whether a team emerged as the champion in a given year. These additional columns provide valuable insights into the success metrics of teams participating in the tournament, enabling us to conduct more comprehensive and nuanced analyses.
Combining Data
In the process of combining data from multiple sources, we leverage concatenation and merging techniques to consolidate disparate datasets into a unified and coherent whole. Through the use of pd.concat, we vertically stack datasets from different years, facilitating cross-year comparisons and trend analysis. Subsequently, we employ pd.merge to horizontally merge the concatenated data with tournament results, enriching the dataset with additional information such as team performance and tournament outcomes. This integrated dataset serves as the foundation for our subsequent analysis, enabling us to examine trends and patterns across multiple dimensions.
Visualization 
Visualization plays a pivotal role in our data exploration and analysis efforts, allowing us to gain insights into complex datasets through intuitive and informative visual representations. Leveraging the Seaborn library, we create a variety of visualizations, including catplots and relplots, to explore the relationship between various statistical categories and team performance in March Madness. These visualizations provide valuable insights into the factors influencing team success, enabling us to identify key performance indicators and trends.
Additionally, we employ graph manipulation techniques to enhance the clarity and effectiveness of our visualizations. By setting a constant color throughout our graphs and utilizing the hue parameter in relplot graphs, we make sure not to have any confounding elements like extra colors in our figures, making it easier to interpret and analyze the data. These visualizations serve as powerful tools, enabling us to better understand the correlation between variables in our dataset.
Prediction and Supervised Machine Learning
In the realm of prediction and supervised machine learning, we leverage the Scikit-learn library to develop predictive models aimed at forecasting team performance in March Madness In our exploration of prediction and supervised machine learning techniques, we incorporated a K-nearest neighbors (KNN) classifier. Through hyperparameter tuning using GridSearchCV, we optimized the KNN model by identifying the optimal number of neighbors for classification. After fitting the model to training data and evaluating its performance on test data, we obtained accuracy scores and determined the most effective number of neighbors for our dataset. Additionally, we generate confusion matrices to visually assess the model's ability to make accurate predictions, providing valuable insights into its strengths and limitations. This thorough evaluation process allowed us to assess the suitability of the KNN algorithm for predicting team performance in March Madness based on relevant features, complementing our logistic regression analysis. 
Section 4: Results and Methods
To gather the T-Rank tables from Bart Torvik, we went to Barttrovik.com, and filtered the home page, which includes the table, by year and regular season. Then, adding “&csv=1” to the website URL downloads the CSV file when clicking enter. To get the 538 tournament results data set, we searched for the dataset on Kaggle and downloaded it straight from there. We then used pandas to open and work with the data sets using “pandas.read_csv”. When downloading the Torvik data sets, the column names were lost, so we had to rename all of the columns and use “.columns” on the datasets to give them the column names. We then used “pd.concat” to combine all of the T-Rank tables, then inner merged on the ‘YEAR’ and ‘TEAM’ columns to get all of the teams in a given year with their respective tournament results. Finally, we dropped the six ‘NA’ columns that we could not find column names for, and created two new columns, ‘CHAMPION’ (by filtered ‘ROUND==1’) and ‘FINAL_FOUR’ (by filtering ‘ROUND’ equal to 1,2, or 4) to get the teams that were champions and those that made the final four. After this scraping and cleaning, we were ready to make our visualizations and models.
A few of the bad predictors of the March Madness tournament outcome are 2P%, 3P%, TORD, and DRB.  When looking at the boxplots (made using the catplot function in the seaborn library), it becomes evident why these are bad predictors for the March Madness outcome.
 
Figure 1 shows the defensive rebound percentage for teams eliminated in each round of the tournament. Surprisingly, there was a lot of overlap in the ranges of all the boxplots, with no clear pattern in the medians. One would assume that better teams, and thus teams that make it farther in the tournament, have a higher DRB than teams that get out earlier. This is because the higher the percentage of rebounds you get on defense, the more opportunities you have to take possession and prevent your opponent from getting second-chance points off an offensive rebound. However, teams that won the championship and made the championship game both had lower median DRB than those that were eliminated earlier. This is quite surprising, as it contradicts our intuitive understanding of the game. Perhaps this can be attributed to a small sample size for teams that went the farthest, but regardless, it shows that defensive rebound percentage is not an important predictor of tournament success.
Figure 2 shows the turnover rate defense, which is calculated by taking the number of turnovers a team forces per 100 opponent possessions. This is an important defensive statistic, as forcing turnovers give a team the ball, many times with an easy fast break score, and shifting the momentum of the game. However, the boxplots are probably the most uniform of any statistic we looked at, with the medians hardly changing and only the range getting smaller as teams went farther in the tournament. This is another surprising result, as it shows that forcing turnovers is not actually a deciding factor in how far a team goes in the tournament. We can conclude that an aggressive defense is not always the best defense, and the large number of outliers compared to the DRB visualization shows that both very aggressive defenses (high turnover rate) and non-aggressive defenses (low turnover rate) can be effective. One champion had an outlier TORD of around 25, and one team that made the championship had an outlier of around 14, showing the effectiveness of both strategies. However, it is important to note that the interquartile ranges for both championship and final 2 teams were very small, perhaps suggesting that teams with defenses somewhere in between aggressive and passive tend to have the most success.
Regardless, in both of these catplots it is evident that, as the rounds progress, there are no major changes in the distribution of data from one round to the next. This makes it difficult to predict tournament success based on defensive rebound percentage or turnover rate on defense.


Figure 5 shows the distribution of team’s adjusted offensive efficiency (ADJOE), which is the amount of points a team would score per 100 possessions against a standard Division 1 team. Looking at the ADJOE values, there is generally a significant positive trend in the median ADJOE values for teams that make it farther in March Madness. This makes logical sense, as “better” teams will score more and thus make it farther in the tournament. The trend of increasing ADJOE with better tournament performance shows that ADJOE is potentially a factor that separates Sweet Sixteen and onward teams from the rest. The median ADJOE for teams in the Sweet Sixteen is roughly 115 points while the median ADJOE for teams that only make it to the Round of 32 is roughly 112. This difference is even steeper when looking at teams that only make it to the Round of 64 where the median team ADJOE is roughly 108. While these differences aren’t very large, basketball is full of close games that are decided by three, two, and even one point margins.
Figure 6 visualizes the distribution of team’s adjusted defensive efficiency (ADJE), which is the number of points allowed per 100 possessions against a typical Division 1 opponent. In the figure, there is generally a downwards trend in the median ADJDE values for teams that make it farther in March Madness. As an inverse of ADJOE, it makes sense that better teams will allow less opponent points and thus make it farther in the tournament. Similar to ADJOE, this negative trend shows that ADJDE is an important factor in determining how far a team is likely to go in March Madness. For example, the median ADJDE for teams that made it to the Round of 64 is around 97 points while that value is roughly 95 points for teams that made it to the Round of 32. Likewise, the distribution is tighter for Round of 32 teams compared to Round of 64 teams.
In order to answer our third research question, the extent to which we can predict a team’s success in March Madness, we utilized a KNN model. The outcome variable for the model was the final round reached by a team, the same as the x-axis variable for our visualizations. We chose to use a KNN model instead of a logistic regression or naive-bayes model because we suspected the relationship between our predictors and outcome could be non-parametric, and KNN handles multiple categories of an outcome variable well. We chose to use the filtered dataset containing teams meeting the Final-Four threshold for offense and defense because without this threshold, our data was overwhelmingly made up of teams that did not advance far in the tournament, so the KNN model did not perform well. This way, we could assess how often high-caliber teams actually end up performing well in the tournament.
The predictors for our model were adjusted offensive efficiency, adjusted defensive efficiency, adjusted tempo, and BARTHAG. We chose these variables because our visualizations suggested they may do a good job of predicting a team’s best round. Using these variables, our KNN had a 37.2% accuracy and used a K = 6 nearest neighbors. While this is higher than the baseline of 14.2% (since there were seven possible outcome categories for these data), it is still not very high. With that being said, the accuracy decreased when we incorporated defensive rebounding, and remained nearly the same when we incorporated turnover rate defense (these variables were suggested to not have much influence by our figures). These results helped support our conclusion that defensive rebounding and turnover rate defense are not very predictive of a team’s success.


We can see that the model most often predicted teams to lose in the round of 64 or 32, and only predicted two teams to win the championship. In general, the model got worse at predicting later rounds in the tournament, but did a better job at recognizing teams that would not advance far. For example, the model correctly predicted 14 teams to lose with 64 teams remaining that actually lost with 64 teams remaining, and it correctly predicted 11 teams to lose with 32 teams remaining.  However, it also predicted two teams to lose with 64 teams remaining that eventually won the championship (or reached the round with 1 team remaining). This shows the limits of the predictive power of our model.
Section 5: Limitations and future work
Our primary limitation was the structure of our data. Since the amount of teams that reach a given round gets cut in half each round, we had very few Final Four and championship teams compared to teams that lost in the first round. This limited our ability to predict teams that reached later rounds because we had less data to work from. So, future work could involve collecting data from more past tournaments in order to expand the number of data points for later rounds.
Another limitation was our knowledge of predictive models. While the KNN model did better than the baseline, a more complex machine learning model could do a better job at predicting our outcome. Future work could also involve developing a more complex model that incorporated more parameters in order to achieve a higher accuracy.
Section 6: Conclusion
From our exploratory data analysis, we found that adjusted offensive efficiency, adjusted defensive efficiency, adjusted tempo, and BARTHAG were most correlated with success in March Madness. We justified this through visualizations that showed strong trends as teams made it farther in the tournament and testing these variables on the classification accuracy of our KNN model. On the contrary, we found that overvalued predictors regarding a team’s March Madness success included 2P%, 3P%, TORD, and DRB, among others. The common theme between these factors is that there is not a strong correlation between a team’s advancement and the improvement of these factors. This isn’t to say that these factors aren’t important to a team’s success, as they still very much are. Through our visualizations and KNN model testing, we found that factors such as these without correlation to the tournament round are not useful predictors. Finally, we created a KNN model to find the degree to which it’s possible to predict a team’s March Madness progress.With a classification accuracy of roughly 37%, we found that it is difficult to predict a team’s farthest round. Additionally, our model performed better for earlier rounds of the tournament and struggled predicting teams in later rounds (Elite Eight, Final Four, Championship).



Section 7: Appendix

In Figure 3, we can see boxplots for two-point percentages, the percentage of two-point shots each team makes over the course of the season. While it does appear that the median values trend up as teams move farther in the tournament, there is still a lot of overlap between the ranges of each of the boxplots. The median of around 54% for champions is still below the 3rd quartile for teams eliminated in the Sweet Sixteen. This doesn’t mean the 2P% is not important, as we can see that no team under 49% reached the championship game. However, it doesn’t appear to be the biggest predictor of success. When added to the K-nearest neighbors model (see more below), two-point percentage did not add anything to the accuracy of the model. It is likely that the small differences in two point percentage is accounted for in the adjusted offensive efficiency metric (teams that make a higher percent of their twos score more points on average).
	
Figure 4 visualizes three-point percentages, the percentage of three-pointers that a team makes in a season. Similar to two-pointers, there is a slight upward trend in the median values as teams move farther in the tournament, but it is very minor. Teams that won the championship had a median of around 37% while those eliminated in the round of 64 had a median of 36%. This is not a large difference at all, bringing into question the predictive power of a three-point percentage. However, one thing to note is that there appears to be a bit of a lower bound on 3P% for teams in the elite eight and beyond, with only three teams (outliers) falling below 33%. So, this could potentially mean that while being an elite three-point shooting team is not important, being at least decent at shooting threes is a common characteristic in teams that make it far in the tournament. Moreover, like the two-point percentage, the point percentage did not add to the predictive power of the KNN model. Likely already accounted for by offensive efficiency, 3P% does not appear to be very helpful in predicting March Madness success. This was a surprising result for us, as many believe basketball has become a game dominated by threes.

